{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.v2 as v2\n",
    "import torchvision.datasets as ds\n",
    "from torchvision.models.resnet import ResNet50_Weights\n",
    "import resnet50\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import SGD, Adam, Adamax\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhieu1001\u001b[0m (\u001b[33mno-organization-123\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = '/home/hieu/Desktop/Computer_Vision/General/classification.ipynb'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of labels\n",
    "# Labels are not equally distributed, need to handle later\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracking performance\n",
    "\n",
    "Config\n",
    "- Hyperparam of data\n",
    "  - batch size\n",
    "- Hyperparam of optimizer:\n",
    "  - learning rate\n",
    "  - epoch\n",
    "- Hyperparam of model:\n",
    "  - pretrained model\n",
    "  - non-freezed layers\n",
    "\n",
    "Metric log (for error analysis)\n",
    "- Prediction:\n",
    "  - Overall: Accuracy, Precision, Recall, F1 score. Micro-averaging and macro averaging, micro-averaging gives equal weight to each class while micro averaging gives equal weight to each instance. Accuracy = (Correct predictions / All predictions): disregards class imbalance and cost of different errors. The most straight forward way is to calculate the Recall and Precision for each class. Recall = (correct class A prediction) / (all class A instances); Precision = (correct class A) / (all class A prediction)\n",
    "- Weights: histogram, norm, \n",
    "- Grad: Histogram of gradient, gradient norm in each layer\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Training Loss: 5.5767\n",
      "Training Loss: 5.4715\n",
      "Training Loss: 5.5497\n",
      "Training Loss: 5.4958\n",
      "Training Loss: 5.4544\n",
      "Training Loss: 5.4222\n",
      "Training Loss: 5.3939\n",
      "Training Loss: 5.4752\n",
      "Training Loss: 5.3393\n",
      "Training Loss: 5.2530\n",
      "Training Loss: 5.3205\n",
      "Training Loss: 5.2185\n",
      "Training Loss: 5.4943\n",
      "Training Loss: 5.3840\n",
      "Training Loss: 5.3000\n",
      "Training Loss: 5.0481\n",
      "Training Loss: 5.2176\n",
      "Training Loss: 5.2675\n",
      "Training Loss: 4.9825\n",
      "Training Loss: 5.0328\n",
      "Training Loss: 4.9556\n",
      "Training Loss: 5.1972\n",
      "Training Loss: 5.2894\n",
      "Training Loss: 5.1819\n",
      "Training Loss: 5.0046\n",
      "Training Loss: 5.0833\n",
      "Training Loss: 5.2850\n",
      "torch.Size([817])\n",
      "torch.Size([817, 257])\n",
      "Testing Loss: 4.2451\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# Hyperparameters\n",
    "config = {\n",
    "    'K': 5,\n",
    "    'SEED': 42,\n",
    "    'ROOT': 'data',\n",
    "    'BATCH': 32,\n",
    "    'NOF_CLASSES': 257,\n",
    "    'TEST_SPLIT': 0.2,\n",
    "    'EPOCH': 20,\n",
    "    'LR': 0.001,\n",
    "    'MODEL': \"resnet50\",\n",
    "    'NON_FREEZE_PARAMS': ['fc.weight', 'fc.bias']\n",
    "}\n",
    "np.random.seed(config[\"SEED\"])\n",
    "\n",
    "# Transform and Augment data, for each batch, right now using\n",
    "# Resize only, can change to resize random crop later.\n",
    "transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.AugMix(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Create dataset, split into training and testing dataset\n",
    "caltech256 = ds.Caltech256(\n",
    "    root= config[\"ROOT\"],\n",
    "    download= False,\n",
    "    transform= transforms\n",
    ")\n",
    "indices = np.arange(0, len(caltech256))\n",
    "np.random.shuffle(indices)\n",
    "test_indices = indices[0: int(len(caltech256) * config[\"TEST_SPLIT\"])]\n",
    "train_indices = indices[int(len(caltech256) * config[\"TEST_SPLIT\"]):]\n",
    "categories = caltech256.categories\n",
    "\n",
    "# Initialize pretrained model, freeze other layer except for last layer\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "process = weights.transforms()\n",
    "\n",
    "# get_state_dict return an ordered_dict\n",
    "# whihc basically means iterate through\n",
    "# this dist is ordered\n",
    "# Popitem(last = True) pop the last item\n",
    "states = weights.get_state_dict()\n",
    "states.popitem()\n",
    "states.popitem()\n",
    "\n",
    "model = resnet50.ResNet(resnet50.Bottleneck, [3, 4, 6, 3], num_classes= config[\"NOF_CLASSES\"])\n",
    "# Strict = False so we do not need to specify fn layer (keep as random)\n",
    "model.load_state_dict(states, strict = False)\n",
    "for name, parameter in model.named_parameters():\n",
    "    if name in states.keys():\n",
    "        parameter.requires_grad = False\n",
    "        \n",
    "# Eval mode do not disable the gradient but only affect\n",
    "# module such as Dropout and BatchNorm\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Main training loop\n",
    "kfold  = KFold(n_splits= 10, shuffle= True, random_state= config[\"SEED\"])\n",
    "\n",
    "for i, (train_index_index, val_index_index) in enumerate(kfold.split(train_indices)):\n",
    "    # Kfold shuffle is really bad, so many same class in one batch\n",
    "    train_index = train_indices[train_index_index]\n",
    "    val_index = train_indices[val_index_index]\n",
    "    # Init dataloader\n",
    "    train_dataloader = DataLoader(caltech256, batch_size= config[\"BATCH\"], sampler= train_index)\n",
    "    val_dataloader = DataLoader(caltech256, batch_size= config[\"BATCH\"], sampler= val_index)\n",
    "    # Init optimizer\n",
    "    optimizer = Adam(model.parameters(), lr = config[\"LR\"])\n",
    "    # Init criterion\n",
    "    criterion = CrossEntropyLoss()\n",
    "    # Init wandb\n",
    "    # wandb.init(\n",
    "    #     project = \"caltech256-classification\",\n",
    "    #     config = config,\n",
    "    #     id= f\"Fold: {i}\"\n",
    "    # )\n",
    "    print(\"Start training\")\n",
    "    train.train(model, criterion, optimizer, val_dataloader, device)\n",
    "    train.test(model, criterion, val_dataloader, categories, \"validation\", device )\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
